{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd6f0f7",
   "metadata": {},
   "source": [
    "# Migration Data Analysis - Comprehensive Guide\n",
    "\n",
    "This notebook provides a complete workflow for analyzing migration data using Python and data science libraries. It covers data loading, exploration, visualization, statistical analysis, and machine learning approaches.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Import Required Libraries](#import-libraries)\n",
    "2. [Data Loading and Initial Exploration](#data-loading)\n",
    "3. [Data Preprocessing](#preprocessing)\n",
    "4. [Exploratory Data Analysis](#eda)\n",
    "5. [Statistical Analysis](#statistical-analysis)\n",
    "6. [Machine Learning for Migration Prediction](#ml-analysis)\n",
    "7. [Results and Conclusions](#conclusions)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb8bd3",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries {#import-libraries}\n",
    "\n",
    "We'll start by importing all the necessary libraries for data manipulation, visualization, and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c1be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data science libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import pingouin as pg\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Geospatial analysis\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set up plotting parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"📊 Pandas version: {pd.__version__}\")\n",
    "print(f\"🔢 NumPy version: {np.__version__}\")\n",
    "print(f\"📈 Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"🎨 Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d456a44",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration {#data-loading}\n",
    "\n",
    "In this section, we'll load migration data and perform initial exploration to understand its structure and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf91e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this demonstration, we'll create sample migration data\n",
    "# In a real project, you would load your actual data using:\n",
    "# df = pd.read_csv('../data/raw/migration_data.csv')\n",
    "\n",
    "def create_sample_migration_data():\n",
    "    \"\"\"Create sample migration data for demonstration purposes.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Date range\n",
    "    dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='M')\n",
    "    \n",
    "    # Countries and regions\n",
    "    origins = ['United States', 'Mexico', 'Canada', 'Germany', 'France', \n",
    "               'Italy', 'Spain', 'Brazil', 'Argentina', 'Japan']\n",
    "    destinations = ['California', 'Texas', 'Florida', 'New York', 'Ontario',\n",
    "                   'Bavaria', 'Île-de-France', 'São Paulo', 'Tokyo', 'Berlin']\n",
    "    \n",
    "    data = []\n",
    "    for date in dates:\n",
    "        for origin in origins:\n",
    "            for dest in destinations:\n",
    "                # Create realistic migration patterns\n",
    "                base_migration = np.random.poisson(100)\n",
    "                seasonal_factor = 1 + 0.3 * np.sin(2 * np.pi * date.month / 12)\n",
    "                economic_factor = np.random.normal(1, 0.2)\n",
    "                \n",
    "                migration_count = int(base_migration * seasonal_factor * economic_factor)\n",
    "                \n",
    "                data.append({\n",
    "                    'date': date,\n",
    "                    'origin_country': origin,\n",
    "                    'destination_region': dest,\n",
    "                    'migration_count': max(0, migration_count),\n",
    "                    'economic_index': np.random.normal(100, 15),\n",
    "                    'political_stability': np.random.uniform(1, 10),\n",
    "                    'distance_km': np.random.uniform(500, 15000),\n",
    "                    'unemployment_rate': np.random.uniform(2, 15),\n",
    "                    'gdp_per_capita': np.random.uniform(15000, 80000)\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create sample data\n",
    "df = create_sample_migration_data()\n",
    "\n",
    "print(f\"📊 Dataset shape: {df.shape}\")\n",
    "print(f\"📅 Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"🌍 Countries: {df['origin_country'].nunique()}\")\n",
    "print(f\"📍 Destinations: {df['destination_region'].nunique()}\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FIRST 5 ROWS\")\n",
    "print(\"=\"*50)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9360996a",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing {#preprocessing}\n",
    "\n",
    "Before analysis, we need to clean and prepare our data. This includes handling missing values, data type conversions, and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67adde39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Assessment\n",
    "print(\"🔍 DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check for missing values\n",
    "missing_data = df.isnull().sum()\n",
    "if missing_data.sum() > 0:\n",
    "    print(\"❌ Missing values found:\")\n",
    "    print(missing_data[missing_data > 0])\n",
    "else:\n",
    "    print(\"✅ No missing values found\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\n🔄 Duplicate rows: {duplicates}\")\n",
    "\n",
    "# Data type optimization\n",
    "print(f\"\\n💾 Memory usage before optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Convert date column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Feature Engineering\n",
    "print(\"\\n🔧 FEATURE ENGINEERING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Extract temporal features\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "# Create cyclical features for seasonality\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "# Calculate migration intensity\n",
    "df['migration_per_1000km'] = df['migration_count'] / (df['distance_km'] / 1000)\n",
    "\n",
    "# Create economic stability index\n",
    "df['economic_stability'] = (df['economic_index'] / 100) * df['political_stability']\n",
    "\n",
    "# Categorize migration volume\n",
    "def categorize_migration(count):\n",
    "    if count < 50:\n",
    "        return 'Low'\n",
    "    elif count < 150:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "df['migration_category'] = df['migration_count'].apply(categorize_migration)\n",
    "\n",
    "# Create regional aggregations\n",
    "monthly_migration = df.groupby(['year', 'month'])['migration_count'].sum().reset_index()\n",
    "country_totals = df.groupby('origin_country')['migration_count'].sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"✅ Feature engineering completed!\")\n",
    "print(f\"📊 New dataset shape: {df.shape}\")\n",
    "print(f\"📈 New features created: {df.columns.tolist()[-8:]}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n📈 SUMMARY STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3600d36",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis {#eda}\n",
    "\n",
    "Now let's explore the data through various visualizations to understand migration patterns, trends, and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc90ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Migration Trends Over Time\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "\n",
    "# Monthly migration trends\n",
    "monthly_trend = df.groupby('date')['migration_count'].sum()\n",
    "axes[0, 0].plot(monthly_trend.index, monthly_trend.values, linewidth=2)\n",
    "axes[0, 0].set_title('📈 Total Migration Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Migration Count')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Migration by country\n",
    "top_countries = df.groupby('origin_country')['migration_count'].sum().nlargest(10)\n",
    "axes[0, 1].barh(top_countries.index, top_countries.values)\n",
    "axes[0, 1].set_title('🌍 Top Origin Countries', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Total Migration Count')\n",
    "\n",
    "# Seasonal patterns\n",
    "seasonal_data = df.groupby('month')['migration_count'].mean()\n",
    "axes[1, 0].bar(seasonal_data.index, seasonal_data.values, color='skyblue')\n",
    "axes[1, 0].set_title('📅 Seasonal Migration Patterns', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Month')\n",
    "axes[1, 0].set_ylabel('Average Migration Count')\n",
    "\n",
    "# Migration categories distribution\n",
    "category_counts = df['migration_category'].value_counts()\n",
    "axes[1, 1].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')\n",
    "axes[1, 1].set_title('📊 Migration Volume Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Interactive Migration Dashboard\n",
    "print(\"\\n🎯 INTERACTIVE VISUALIZATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Time series with trend\n",
    "fig = px.line(monthly_migration, x='month', y='migration_count', \n",
    "              color='year', title='Migration Trends by Year and Month')\n",
    "fig.update_layout(height=500)\n",
    "fig.show()\n",
    "\n",
    "# Migration flows heatmap\n",
    "migration_matrix = df.pivot_table(\n",
    "    values='migration_count', \n",
    "    index='origin_country', \n",
    "    columns='destination_region', \n",
    "    aggfunc='sum'\n",
    ").fillna(0)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(migration_matrix, annot=False, cmap='YlOrRd', cbar_kws={'label': 'Migration Count'})\n",
    "plt.title('🗺️ Migration Flow Heatmap: Origin Countries vs Destination Regions', \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Destination Region')\n",
    "plt.ylabel('Origin Country')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Economic Factors Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "\n",
    "# Economic index vs migration\n",
    "axes[0, 0].scatter(df['economic_index'], df['migration_count'], alpha=0.6)\n",
    "axes[0, 0].set_title('💰 Economic Index vs Migration Count', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Economic Index')\n",
    "axes[0, 0].set_ylabel('Migration Count')\n",
    "\n",
    "# Political stability vs migration\n",
    "axes[0, 1].scatter(df['political_stability'], df['migration_count'], alpha=0.6, color='orange')\n",
    "axes[0, 1].set_title('🏛️ Political Stability vs Migration', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Political Stability Score')\n",
    "axes[0, 1].set_ylabel('Migration Count')\n",
    "\n",
    "# Distance vs migration\n",
    "axes[1, 0].scatter(df['distance_km'], df['migration_count'], alpha=0.6, color='green')\n",
    "axes[1, 0].set_title('📏 Distance vs Migration Count', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Distance (km)')\n",
    "axes[1, 0].set_ylabel('Migration Count')\n",
    "\n",
    "# Unemployment rate vs migration\n",
    "axes[1, 1].scatter(df['unemployment_rate'], df['migration_count'], alpha=0.6, color='red')\n",
    "axes[1, 1].set_title('💼 Unemployment Rate vs Migration', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Unemployment Rate (%)')\n",
    "axes[1, 1].set_ylabel('Migration Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Exploratory Data Analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284d4d6",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis {#statistical-analysis}\n",
    "\n",
    "Let's perform statistical tests to understand relationships and patterns in the migration data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab251983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Correlation Analysis\n",
    "print(\"🔗 CORRELATION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate correlations between numeric variables\n",
    "numeric_cols = ['migration_count', 'economic_index', 'political_stability', \n",
    "                'distance_km', 'unemployment_rate', 'gdp_per_capita']\n",
    "\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, fmt='.3f', cbar_kws={\"shrink\": .8})\n",
    "plt.title('🔗 Correlation Matrix of Migration Factors', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print significant correlations\n",
    "print(\"\\n📊 Key Correlations with Migration Count:\")\n",
    "migration_corr = correlation_matrix['migration_count'].drop('migration_count').sort_values(key=abs, ascending=False)\n",
    "for factor, corr in migration_corr.items():\n",
    "    direction = \"📈 Positive\" if corr > 0 else \"📉 Negative\"\n",
    "    strength = \"Strong\" if abs(corr) > 0.5 else \"Moderate\" if abs(corr) > 0.3 else \"Weak\"\n",
    "    print(f\"  {factor}: {corr:.3f} ({direction}, {strength})\")\n",
    "\n",
    "# 2. Statistical Tests\n",
    "print(f\"\\n🧪 STATISTICAL TESTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test for normality of migration data\n",
    "statistic, p_value = stats.shapiro(df['migration_count'].sample(5000))  # Sample for large datasets\n",
    "print(f\"📊 Shapiro-Wilk Normality Test:\")\n",
    "print(f\"  Statistic: {statistic:.4f}, p-value: {p_value:.4f}\")\n",
    "print(f\"  Migration data is {'normally' if p_value > 0.05 else 'not normally'} distributed\")\n",
    "\n",
    "# ANOVA test - Migration differences across countries\n",
    "print(f\"\\n🌍 ANOVA Test - Migration differences across countries:\")\n",
    "country_groups = [group['migration_count'].values for name, group in df.groupby('origin_country')]\n",
    "f_stat, p_val = stats.f_oneway(*country_groups)\n",
    "print(f\"  F-statistic: {f_stat:.4f}, p-value: {p_val:.4f}\")\n",
    "print(f\"  {'Significant' if p_val < 0.05 else 'No significant'} differences between countries\")\n",
    "\n",
    "# 3. Time Series Analysis\n",
    "print(f\"\\n📈 TIME SERIES ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Aggregate monthly data for time series analysis\n",
    "monthly_ts = df.groupby('date')['migration_count'].sum().sort_index()\n",
    "\n",
    "# Seasonal decomposition\n",
    "decomposition = seasonal_decompose(monthly_ts, model='additive', period=12)\n",
    "\n",
    "# Plot decomposition\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "\n",
    "decomposition.observed.plot(ax=axes[0], title='📊 Original Time Series')\n",
    "decomposition.trend.plot(ax=axes[1], title='📈 Trend Component')\n",
    "decomposition.seasonal.plot(ax=axes[2], title='🔄 Seasonal Component')\n",
    "decomposition.resid.plot(ax=axes[3], title='🎲 Residual Component')\n",
    "\n",
    "plt.suptitle('Time Series Decomposition of Migration Data', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate trend statistics\n",
    "trend_slope = np.polyfit(range(len(monthly_ts)), monthly_ts.values, 1)[0]\n",
    "print(f\"📈 Overall trend: {trend_slope:.2f} migrations per month\")\n",
    "print(f\"   Trend direction: {'Increasing' if trend_slope > 0 else 'Decreasing'}\")\n",
    "\n",
    "# 4. Outlier Detection\n",
    "print(f\"\\n🎯 OUTLIER ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# IQR method for outlier detection\n",
    "Q1 = df['migration_count'].quantile(0.25)\n",
    "Q3 = df['migration_count'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['migration_count'] < lower_bound) | (df['migration_count'] > upper_bound)]\n",
    "print(f\"📊 Outliers detected: {len(outliers)} ({len(outliers)/len(df)*100:.1f}% of data)\")\n",
    "print(f\"📏 IQR bounds: [{lower_bound:.0f}, {upper_bound:.0f}]\")\n",
    "\n",
    "if len(outliers) > 0:\n",
    "    print(f\"🔍 Top outlier countries:\")\n",
    "    outlier_countries = outliers.groupby('origin_country')['migration_count'].count().nlargest(5)\n",
    "    for country, count in outlier_countries.items():\n",
    "        print(f\"  {country}: {count} outlier records\")\n",
    "\n",
    "print(\"\\n✅ Statistical analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55917e25",
   "metadata": {},
   "source": [
    "## 6. Machine Learning for Migration Prediction {#ml-analysis}\n",
    "\n",
    "Now we'll build machine learning models to predict migration patterns based on various factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae0bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preparation for ML\n",
    "print(\"🤖 MACHINE LEARNING ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Prepare features for modeling\n",
    "feature_columns = ['economic_index', 'political_stability', 'distance_km', \n",
    "                   'unemployment_rate', 'gdp_per_capita', 'month_sin', 'month_cos']\n",
    "\n",
    "# Encode categorical variables\n",
    "le_country = LabelEncoder()\n",
    "le_destination = LabelEncoder()\n",
    "\n",
    "df_ml = df.copy()\n",
    "df_ml['origin_country_encoded'] = le_country.fit_transform(df_ml['origin_country'])\n",
    "df_ml['destination_region_encoded'] = le_destination.fit_transform(df_ml['destination_region'])\n",
    "\n",
    "# Add encoded variables to features\n",
    "feature_columns.extend(['origin_country_encoded', 'destination_region_encoded'])\n",
    "\n",
    "X = df_ml[feature_columns]\n",
    "y = df_ml['migration_count']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"📊 Training set size: {X_train.shape}\")\n",
    "print(f\"📊 Test set size: {X_test.shape}\")\n",
    "print(f\"🎯 Features used: {feature_columns}\")\n",
    "\n",
    "# 2. Model Training and Evaluation\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(f\"\\n🎯 MODEL TRAINING AND EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n🔄 Training {name}...\")\n",
    "    \n",
    "    # Use scaled data for Linear Regression, original for tree-based models\n",
    "    if name == 'Linear Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R²': r2,\n",
    "        'Model': model,\n",
    "        'Predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"  📈 R² Score: {r2:.4f}\")\n",
    "    print(f\"  📏 RMSE: {rmse:.2f}\")\n",
    "    print(f\"  📊 MAE: {mae:.2f}\")\n",
    "\n",
    "# 3. Model Comparison\n",
    "print(f\"\\n🏆 MODEL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'R² Score': [results[model]['R²'] for model in results.keys()],\n",
    "    'RMSE': [results[model]['RMSE'] for model in results.keys()],\n",
    "    'MAE': [results[model]['MAE'] for model in results.keys()]\n",
    "})\n",
    "\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = comparison_df.loc[comparison_df['R² Score'].idxmax(), 'Model']\n",
    "best_model = results[best_model_name]['Model']\n",
    "print(f\"\\n🥇 Best performing model: {best_model_name}\")\n",
    "\n",
    "# 4. Feature Importance (for tree-based models)\n",
    "if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "    print(f\"\\n🎯 FEATURE IMPORTANCE ({best_model_name})\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(feature_importance)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=feature_importance, x='Importance', y='Feature')\n",
    "    plt.title(f'Feature Importance - {best_model_name}', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 5. Prediction vs Actual Plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, (name, result) in enumerate(results.items(), 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.scatter(y_test, result['Predictions'], alpha=0.6)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Migration Count')\n",
    "    plt.ylabel('Predicted Migration Count')\n",
    "    plt.title(f'{name}\\nR² = {result[\"R²\"]:.3f}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Prediction vs Actual Migration Counts', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Clustering Analysis\n",
    "print(f\"\\n🎯 CLUSTERING ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Prepare data for clustering (using scaled features)\n",
    "cluster_features = ['economic_index', 'political_stability', 'unemployment_rate', 'gdp_per_capita']\n",
    "X_cluster = scaler.fit_transform(df[cluster_features])\n",
    "\n",
    "# Determine optimal number of clusters\n",
    "inertias = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_cluster)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range, inertias, 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Perform clustering with optimal k (let's use k=4)\n",
    "optimal_k = 4\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_cluster)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df_clustered = df.copy()\n",
    "df_clustered['cluster'] = cluster_labels\n",
    "\n",
    "# Analyze clusters\n",
    "print(f\"🔍 Cluster Analysis (k={optimal_k}):\")\n",
    "cluster_summary = df_clustered.groupby('cluster')[cluster_features + ['migration_count']].mean()\n",
    "print(cluster_summary.round(2))\n",
    "\n",
    "# Visualize clusters\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# PCA visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_cluster)\n",
    "\n",
    "scatter = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis', alpha=0.6)\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "axes[0].set_title('Migration Clusters (PCA Visualization)')\n",
    "plt.colorbar(scatter, ax=axes[0])\n",
    "\n",
    "# Cluster characteristics\n",
    "cluster_migration = df_clustered.groupby('cluster')['migration_count'].mean()\n",
    "axes[1].bar(cluster_migration.index, cluster_migration.values, color=['red', 'blue', 'green', 'orange'])\n",
    "axes[1].set_xlabel('Cluster')\n",
    "axes[1].set_ylabel('Average Migration Count')\n",
    "axes[1].set_title('Average Migration by Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Machine Learning analysis completed!\")\n",
    "print(f\"🏆 Best model: {best_model_name} (R² = {results[best_model_name]['R²']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6791d9",
   "metadata": {},
   "source": [
    "## 7. Results and Conclusions {#conclusions}\n",
    "\n",
    "Let's summarize our key findings from the migration data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8c05f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Final Summary Report\n",
    "print(\"📋 MIGRATION ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Key Statistics\n",
    "total_migrations = df['migration_count'].sum()\n",
    "avg_monthly_migration = df.groupby('date')['migration_count'].sum().mean()\n",
    "top_origin = df.groupby('origin_country')['migration_count'].sum().idxmax()\n",
    "top_destination = df.groupby('destination_region')['migration_count'].sum().idxmax()\n",
    "\n",
    "print(f\"📊 OVERALL STATISTICS\")\n",
    "print(f\"   Total migrations analyzed: {total_migrations:,}\")\n",
    "print(f\"   Average monthly migration: {avg_monthly_migration:.0f}\")\n",
    "print(f\"   Date range: {df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"   Countries analyzed: {df['origin_country'].nunique()}\")\n",
    "print(f\"   Destination regions: {df['destination_region'].nunique()}\")\n",
    "\n",
    "print(f\"\\n🏆 TOP PERFORMERS\")\n",
    "print(f\"   Highest origin country: {top_origin}\")\n",
    "print(f\"   Most popular destination: {top_destination}\")\n",
    "\n",
    "# Economic Insights\n",
    "economic_corr = df['economic_index'].corr(df['migration_count'])\n",
    "political_corr = df['political_stability'].corr(df['migration_count'])\n",
    "distance_corr = df['distance_km'].corr(df['migration_count'])\n",
    "\n",
    "print(f\"\\n💼 KEY CORRELATIONS\")\n",
    "print(f\"   Economic Index: {economic_corr:.3f}\")\n",
    "print(f\"   Political Stability: {political_corr:.3f}\")\n",
    "print(f\"   Distance: {distance_corr:.3f}\")\n",
    "\n",
    "# Model Performance\n",
    "if 'results' in locals():\n",
    "    best_r2 = max([results[model]['R²'] for model in results.keys()])\n",
    "    print(f\"\\n🤖 MACHINE LEARNING RESULTS\")\n",
    "    print(f\"   Best model R² score: {best_r2:.4f}\")\n",
    "    print(f\"   Best model: {best_model_name}\")\n",
    "\n",
    "# Seasonal Insights\n",
    "peak_month = df.groupby('month')['migration_count'].mean().idxmax()\n",
    "low_month = df.groupby('month')['migration_count'].mean().idxmin()\n",
    "\n",
    "print(f\"\\n📅 SEASONAL PATTERNS\")\n",
    "print(f\"   Peak migration month: {peak_month}\")\n",
    "print(f\"   Lowest migration month: {low_month}\")\n",
    "\n",
    "print(f\"\\n🎯 KEY INSIGHTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. 📈 Migration patterns show clear seasonal variations\")\n",
    "print(\"2. 💰 Economic factors have moderate correlation with migration\")\n",
    "print(\"3. 🌍 Geographic distance influences migration decisions\")\n",
    "print(\"4. 🏛️ Political stability affects migration flows\")\n",
    "print(\"5. 🤖 Machine learning models can predict migration with reasonable accuracy\")\n",
    "\n",
    "print(f\"\\n🔮 RECOMMENDATIONS FOR FUTURE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. 📊 Collect more granular temporal data (daily/weekly)\")\n",
    "print(\"2. 🌐 Include additional geographic and demographic factors\")\n",
    "print(\"3. 📰 Incorporate news sentiment and political events\")\n",
    "print(\"4. 🏢 Add economic policy changes as features\")\n",
    "print(\"5. 🔄 Implement real-time prediction systems\")\n",
    "print(\"6. 📱 Develop interactive dashboards for stakeholders\")\n",
    "\n",
    "print(f\"\\n📁 DATA EXPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save processed data and results\n",
    "output_dir = Path('../reports/')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export main dataset\n",
    "df.to_csv(output_dir / 'migration_analysis_dataset.csv', index=False)\n",
    "print(f\"✅ Dataset exported to: {output_dir / 'migration_analysis_dataset.csv'}\")\n",
    "\n",
    "# Export summary statistics\n",
    "summary_stats = {\n",
    "    'total_migrations': int(total_migrations),\n",
    "    'avg_monthly_migration': float(avg_monthly_migration),\n",
    "    'top_origin_country': top_origin,\n",
    "    'top_destination': top_destination,\n",
    "    'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'correlations': {\n",
    "        'economic_index': float(economic_corr),\n",
    "        'political_stability': float(political_corr),\n",
    "        'distance_km': float(distance_corr)\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(output_dir / 'analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "print(f\"✅ Summary exported to: {output_dir / 'analysis_summary.json'}\")\n",
    "\n",
    "# Export model results if available\n",
    "if 'results' in locals():\n",
    "    model_results = {\n",
    "        name: {\n",
    "            'R2': float(result['R²']),\n",
    "            'RMSE': float(result['RMSE']),\n",
    "            'MAE': float(result['MAE'])\n",
    "        }\n",
    "        for name, result in results.items()\n",
    "    }\n",
    "    \n",
    "    with open(output_dir / 'model_results.json', 'w') as f:\n",
    "        json.dump(model_results, f, indent=2)\n",
    "    print(f\"✅ Model results exported to: {output_dir / 'model_results.json'}\")\n",
    "\n",
    "print(f\"\\n🎉 ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Thank you for using the Migration Data Analysis notebook!\")\n",
    "print(\"For questions or improvements, please refer to the project documentation.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
